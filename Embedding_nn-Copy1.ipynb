{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入格式\n",
    "\n",
    "\"\"\"\n",
    "input : {\"creative_id\":\"1,2,3,4,5\",\n",
    "        \"ad_id\":\"1,2,3,4,5\",\n",
    "        \"advertiser_id\":\"1,2,3,4,5\",\n",
    "        \"product_id\":\"1,2,3,4,5\",\n",
    "        \"product_category\":\"1,2,3,4,5\",\n",
    "        \"industry\":\"1,2,3,4,5\"}\n",
    "        \n",
    "label1 : age  sigmoid\n",
    "label2 : gender softmax\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 加载数据\n",
    "df_ad,df_click_log,df_user,df_click_log_test = loadData(\"./train/\",mode=\"pro\")\n",
    "\n",
    "# 整合宽表\n",
    "df_total_train = df_click_log.merge(df_user, how='left', on=\"user_id\")\n",
    "df_total_train = df_total_train.merge(df_ad, how='left', on=\"creative_id\")\n",
    "df_total_train = df_total_train.fillna(\"UNK\").replace([\"\\\\N\"],\"UNK\")\n",
    "df_total_train.head(5)\n",
    "\n",
    "df_total_test = df_click_log_test.merge(df_ad, how='left', on=\"creative_id\")\n",
    "df_total_test = df_total_test.fillna(\"UNK\").replace([\"\\\\N\"],\"UNK\")\n",
    "df_total_test.head(5)\n",
    "\n",
    "# 设定参数\n",
    "features = [\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\"]\n",
    "SEQ_LEN = 45\n",
    "negsample = 0\n",
    "feature_max_idx = {}\n",
    "\n",
    "dict_table = {}\n",
    "max_num = {}\n",
    "\n",
    "for key in features:\n",
    "    ll = [df_total_train[key].tolist(),df_total_test[key].tolist()]\n",
    "    table,_table = encode(ll)\n",
    "    df_total_train[key] = transform(df_total_train[key],table)\n",
    "    df_total_test[key] = transform(df_total_test[key],table)\n",
    "    dict_table[key] = table\n",
    "    max_num[key] = max(dict_table[key].values())\n",
    "    \n",
    "pickle.dump(dict_table,open(base_path+\"data.table\",'wb'))\n",
    "#dict_table = pickle.load(open(\"./\"+method+\"/data.table\",'rb'))\n",
    "\n",
    "data_train = df_total_train.sort_values(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train = pd.read_csv(\"dataTrain.csv\")\n",
    "data_train = pd.read_csv(\"dataTest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train[\"age\"] = data_train[\"age\"] - 1\n",
    "data_train[\"gender\"] = data_train[\"gender\"] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入数据格式 \"age#gender#cseq#adseq#pseq#pcseq#aseq#iseq\"\n",
    "features = [\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\"]\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "with open(base_path+\"tfdata.train\",\"w\") as f,open(base_path+\"tfdata.val\",\"w\") as f1:\n",
    "    for indx,hist in data_train.groupby(\"user_id\"):\n",
    "        age = str(hist[\"age\"].max())\n",
    "        gender = str(hist[\"gender\"].max())\n",
    "        s = [age,gender]\n",
    "        for item in features:\n",
    "            ll = hist[item].astype(str)\n",
    "            llist = pad_sequences([ll], maxlen=60, padding='post', value='0',dtype=object).tolist()[0]\n",
    "            # list to str\n",
    "            s.append(\",\".join(llist))\n",
    "        if random.random() >= test_size:\n",
    "            f.write(\"#\".join(s)+\"\\n\")\n",
    "        else:\n",
    "            f1.write(\"#\".join(s)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(base_path+\"tfdata.test\",\"w\") as f:\n",
    "    for indx,hist in df_total_test.groupby(\"user_id\"):\n",
    "        s = []\n",
    "        for item in features:\n",
    "            ll = hist[item].astype(str)\n",
    "            llist = pad_sequences([ll], maxlen=60, padding='post', value='0',dtype=object).tolist()[0]\n",
    "            # list to str\n",
    "            s.append(\",\".join(llist))\n",
    "        f.write(\"#\".join(s)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils.utils import loadData,encode,transform,getSeq,getUserAvgSeq,getUserAvgSeq,upload\n",
    "from model.W2V import wvmodel\n",
    "from model.LGB import base_predict,base_train\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import encode,transform\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "method = \"test_nn_seq\"\n",
    "base_path = \"./test_nn_seq/\"\n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.exists(\"./\"+method):\n",
    "    os.mkdir(\"./\"+method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setConfig(dynamic_alloc=True,gpu_no=\"1\",save_checkpoints_steps=1000,print_steps=1000,seed=2020):\n",
    "    gpu_config = tf.ConfigProto(\n",
    "                        log_device_placement=True,\n",
    "                        inter_op_parallelism_threads=0,\n",
    "                        intra_op_parallelism_threads=0,\n",
    "                        allow_soft_placement=True)\n",
    "    gpu_config.gpu_options.visible_device_list = gpu_no\n",
    "    gpu_config.gpu_options.allow_growth = dynamic_alloc\n",
    "    #gpu_config.gpu_options.allocator_type = 'BFC'\n",
    "    run_config = tf.estimator.RunConfig().replace(save_checkpoints_steps=save_checkpoints_steps,\n",
    "                                                  tf_random_seed=seed,\n",
    "                                                  session_config=gpu_config,\n",
    "                                                  log_step_count_steps=print_steps,\n",
    "                                                  save_summary_steps=10000,\n",
    "                                                  keep_checkpoint_max=20)\n",
    "    return run_config\n",
    "\n",
    "\n",
    "class EMNN:\n",
    "    \n",
    "    def __init__(self, config, params):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        global fparams\n",
    "        fparams = params\n",
    "    \n",
    "    def input_fn(self, filenames, batch_size=32, num_epochs=1, perform_shuffle=False):\n",
    "        print('Parsing', filenames)\n",
    "        features = [\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\"]\n",
    "        \n",
    "        def process (columns,features):\n",
    "            res = []\n",
    "            for indx in range(2,len(features)+2):\n",
    "                ids = tf.string_split([columns.values[indx]],\",\")\n",
    "                ids = tf.reshape(ids.values, ids.dense_shape)\n",
    "                ids = tf.string_to_number(ids, out_type=tf.int32)\n",
    "                res.append(ids)\n",
    "            return dict(zip(features,res))\n",
    "        \n",
    "        def decode_libsvm(line):\n",
    "            columns = tf.string_split([line], '#')\n",
    "            age = tf.string_to_number(columns.values[0], out_type=tf.float32)\n",
    "            gender = tf.string_to_number(columns.values[1], out_type=tf.float32)\n",
    "            feature = process(columns,features)\n",
    "            labels = {\"age\":age,\"gender\":gender}\n",
    "            return feature,labels\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(filenames).map(decode_libsvm, num_parallel_calls=10).prefetch(500000)\n",
    "        if perform_shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=256)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        batch_features, batch_labels = iterator.get_next()\n",
    "        return batch_features,batch_labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def Pooling(input_tensor, axis=2, method=\"sum\", name=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        input_tensor - tensor with any shape\n",
    "        axis - the axis to operate\n",
    "        method - the operation to process on the choosed axis\n",
    "\n",
    "        Output:\n",
    "        tensor\n",
    "        \"\"\"\n",
    "        if axis not in list(range(len(input_tensor.shape))):\n",
    "            raise ValueError(\n",
    "                \"Unexpected axis %d, axis expect to include in {}\".format(list(range(len(input_tensor.shape)))) % axis)\n",
    "        if method == \"min\":\n",
    "            pol = tf.reduce_min(input_tensor, axis=axis)\n",
    "        elif method == \"avg\":\n",
    "            pol = tf.reduce_mean(input_tensor, axis=axis)\n",
    "        elif method == \"max\":\n",
    "            pol = tf.reduce_max(input_tensor, axis=axis)\n",
    "        else:\n",
    "            pol = tf.reduce_sum(input_tensor, axis=axis)\n",
    "        return pol\n",
    "    \n",
    "    \n",
    "    def model_fn(self,features, labels, mode,params):\n",
    "        \n",
    "        \"\"\"Bulid Model function f(x) for Estimator.\"\"\"\n",
    "        \n",
    "        ## feature_size ##\n",
    "        # [\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\"]\n",
    "        \n",
    "        # creative_size \n",
    "        creative_size = params[\"creative_size\"] + 1\n",
    "        # ad_size \n",
    "        ad_size = params[\"ad_size\"] + 1\n",
    "        # product_size \n",
    "        product_size = params[\"product_size\"] + 1\n",
    "        # product_category_size\n",
    "        product_category_size = params[\"product_category_size\"] + 1\n",
    "        # advertiser_size\n",
    "        advertiser_size = params[\"advertiser_size\"] + 1\n",
    "        # industry_size \n",
    "        industry_size = params[\"industry_size\"] + 1\n",
    "        \n",
    "        ## embedding_size ##\n",
    "        \n",
    "        # creative_size \n",
    "        creative_emb_size = params[\"creative_emb_size\"]\n",
    "        # ad_size \n",
    "        ad_emb_size = params[\"ad_emb_size\"]\n",
    "        # product_size \n",
    "        product_emb_size = params[\"product_emb_size\"]\n",
    "        # product_category_size\n",
    "        product_category_emb_size = params[\"product_category_emb_size\"]\n",
    "        # advertiser_size\n",
    "        advertiser_emb_size = params[\"advertiser_emb_size\"]\n",
    "        # industry_size \n",
    "        industry_emb_size = params[\"industry_emb_size\"]\n",
    "        \n",
    "        ## attention_size ##\n",
    "        \n",
    "        # a_size\n",
    "        a_size = params[\"a_size\"]\n",
    "        \n",
    "        ## training parameter ## \n",
    "        seq_len = params[\"seq_len\"]\n",
    "        b_layers = params[\"b_layers\"]\n",
    "        t_layers = params[\"t_layers\"]\n",
    "        l2_reg = params[\"l2_reg\"]\n",
    "        learning_rate = params[\"learning_rate\"]\n",
    "\n",
    "        ## bulid embedding weights ##\n",
    "        \n",
    "        embw_cre = tf.get_variable( \n",
    "            name='creative_embedding', \n",
    "            shape=[creative_size,creative_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        embw_ad = tf.get_variable(\n",
    "            name='ad_embedding', \n",
    "            shape=[ad_size,ad_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        embw_pro = tf.get_variable(\n",
    "            name='product_embedding', \n",
    "            shape=[product_size,product_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        embw_pcate = tf.get_variable(\n",
    "            name='pcate_embedding', \n",
    "            shape=[product_category_size,product_category_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        embw_adver = tf.get_variable(\n",
    "            name='adver_embedding', \n",
    "            shape=[advertiser_size,advertiser_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        embw_ind = tf.get_variable(\n",
    "            name='industry_embedding', \n",
    "            shape=[industry_size,industry_emb_size], \n",
    "            initializer=tf.glorot_normal_initializer())\n",
    "                   \n",
    "        ## load features ## \n",
    "        # [\"creative_id\",\"ad_id\",\"product_id\",\"product_category\",\"advertiser_id\",\"industry\"]\n",
    "                   \n",
    "        cre_ids = tf.reshape(features[\"creative_id\"],[-1,seq_len])\n",
    "        ad_ids =  tf.reshape(features[\"ad_id\"],[-1,seq_len])\n",
    "        pro_ids = tf.reshape(features[\"product_id\"],[-1,seq_len])\n",
    "        pro_cate_ids = tf.reshape(features[\"product_category\"],[-1,seq_len])\n",
    "        adv_ids = tf.reshape(features[\"advertiser_id\"],[-1,seq_len])\n",
    "        ind_ids = tf.reshape(features[\"industry\"],[-1,seq_len])\n",
    "        age = labels[\"age\"]\n",
    "        age = tf.one_hot(tf.cast(age,tf.int32),params[\"num_class\"])\n",
    "        gender = labels[\"gender\"]\n",
    "        \n",
    "        ## embedding ##\n",
    "        cre_embedding = tf.nn.embedding_lookup(embw_cre, cre_ids) # None * num * emb_size \n",
    "        ad_embedding = tf.nn.embedding_lookup(embw_ad,ad_ids)\n",
    "        pro_embedding = tf.nn.embedding_lookup(embw_pro,pro_ids)\n",
    "        pcate_embedding = tf.nn.embedding_lookup(embw_pcate,pro_cate_ids)\n",
    "        adv_embedding = tf.nn.embedding_lookup(embw_adver,adv_ids)\n",
    "        ind_embedding = tf.nn.embedding_lookup(embw_ind,ind_ids) \n",
    "                   \n",
    "        ## pooling ##  \n",
    "        cre_embedding = EMNN.Pooling(cre_embedding, axis=1, method=\"avg\") # None * 1 * emb_size \n",
    "        ad_embedding = EMNN.Pooling(ad_embedding, axis=1, method=\"avg\")\n",
    "        pro_embedding = EMNN.Pooling(pro_embedding, axis=1, method=\"avg\")\n",
    "        pcate_embedding = EMNN.Pooling(pcate_embedding, axis=1, method=\"avg\")\n",
    "        adv_embedding = EMNN.Pooling(adv_embedding, axis=1, method=\"avg\")\n",
    "        ind_embedding = EMNN.Pooling(ind_embedding, axis=1, method=\"avg\")\n",
    "                \n",
    "        ## build input for DNN ##\n",
    "        deep_inputs = tf.concat([cre_embedding,\n",
    "                                 ad_embedding,\n",
    "                                 pro_embedding,\n",
    "                                 pcate_embedding,\n",
    "                                 adv_embedding,\n",
    "                                 ind_embedding],-1)\n",
    "\n",
    "        with tf.variable_scope(\"shallow_layer\"):\n",
    "            for i,l in enumerate(b_layers):\n",
    "                deep_inputs = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=l,activation_fn=tf.nn.relu,\n",
    "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg), \n",
    "                                                scope='mlp%d' % i)\n",
    "        \n",
    "        ## output layer 1 ##\n",
    "        with tf.variable_scope(\"gender_layer\"):\n",
    "            h1 = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=t_layers,activation_fn=tf.nn.relu,\n",
    "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"gender_out\"):\n",
    "            out1 = tf.contrib.layers.fully_connected(inputs=h1, num_outputs=1,activation_fn=tf.identity,\n",
    "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "                   \n",
    "        ## output layer 2 ##\n",
    "        with tf.variable_scope(\"age_layer\"):\n",
    "            h2 = tf.contrib.layers.fully_connected(inputs=deep_inputs, num_outputs=t_layers,activation_fn=tf.nn.relu,\n",
    "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope(\"age_out\"):\n",
    "            out2 = tf.contrib.layers.fully_connected(inputs=h2, num_outputs=10,activation_fn=tf.identity,\n",
    "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "            \n",
    "            \n",
    "        pred_gender_logit = tf.reshape(out1,shape=[-1])\n",
    "        pred_age_logit = out2\n",
    "        pred_gender = tf.nn.sigmoid(pred_gender_logit)\n",
    "        pred_age = tf.arg_max(tf.nn.softmax(pred_age_logit),0)\n",
    "        predictions={\"age\": pred_age,\"gender\":pred_gender}\n",
    "                   \n",
    "        export_outputs = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: tf.estimator.export.PredictOutput(predictions)}\n",
    "        # Provide an estimator spec for `ModeKeys.PREDICT`\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=predictions,\n",
    "                export_outputs=export_outputs)\n",
    "\n",
    "        #------bulid loss------\n",
    "                   \n",
    "        age_loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=age,logits=pred_age_logit))\n",
    "                   \n",
    "        gender_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=gender,logits=pred_gender_logit))\n",
    "                \n",
    "        loss = 0.4 * gender_loss + 0.6 * age_loss\n",
    "               #l2_reg * tf.nn.l2_loss(W_mlp_user) + \\\n",
    "        \n",
    "\n",
    "        # Provide an estimator spec for `ModeKeys.EVAL`\n",
    "        eval_metric_ops = {\n",
    "            \"acc_age\":tf.compat.v1.metrics.accuracy(tf.arg_max(age,0),pred_age),\n",
    "            \"auc_gender\":tf.compat.v1.metrics.auc(gender, pred_gender)\n",
    "        }\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "        #------bulid optimizer------\n",
    "        if params[\"optimizer\"] == 'Adam':\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "        elif params[\"optimizer\"] == 'Adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate, initial_accumulator_value=1e-8)\n",
    "        elif params[\"optimizer\"] == 'Momentum':\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95)\n",
    "        elif params[\"optimizer\"] == 'ftrl':\n",
    "            optimizer = tf.train.FtrlOptimizer(learning_rate)\n",
    "\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Provide an estimator spec for `ModeKeys.TRAIN` modes\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions=predictions,\n",
    "                loss=loss,\n",
    "                train_op=train_op)\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "                   \n",
    "    def compile(self,clear_existing_model=True):\n",
    "        run_config = self.config\n",
    "        model_dir = './tmp/checkpoint/{}'.format(self.__class__.__name__)\n",
    "        if clear_existing_model:\n",
    "            try:\n",
    "                shutil.rmtree(model_dir)\n",
    "            except Exception as e:\n",
    "                print(e, \"at clear_existing_model\")\n",
    "        else:\n",
    "            print(\"existing model cleaned at {}\".format(model_dir))\n",
    "        self.model = tf.estimator.Estimator(model_fn=self.model_fn, model_dir=model_dir, params=fparams,\n",
    "                                            config=run_config)\n",
    "\n",
    "    def train(self, tr_files, va_files):\n",
    "        self.model.train(input_fn=lambda: self.input_fn(tr_files, num_epochs=fparams[\"num_epochs\"],\n",
    "                                                        batch_size=fparams[\"batch_size\"]))\n",
    "\n",
    "    def evaluate(self, va_files):\n",
    "        self.model.evaluate(input_fn=lambda: self.input_fn(va_files, num_epochs=1, batch_size=fparams[\"batch_size\"]))\n",
    "\n",
    "    def train_and_evaluate(self, tr_files, va_files):\n",
    "        evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(\n",
    "            estimator=self.model,\n",
    "            input_fn=lambda: self.input_fn(va_files, num_epochs=1, batch_size = fparams[\"batch_size\"]),\n",
    "            every_n_iter = fparams[\"val_itrs\"])\n",
    "        self.model.train(\n",
    "            input_fn=lambda: self.input_fn(tr_files, num_epochs= fparams[\"num_epochs\"],\n",
    "                                           batch_size= fparams[\"batch_size\"]),\n",
    "            hooks=[evaluator])\n",
    "\n",
    "    def predict(self, te_files, isSave=False, numToSave=10):\n",
    "        P_G = self.model.predict(input_fn=lambda: self.input_fn(te_files, num_epochs=1, batch_size=1),\n",
    "                                 predict_keys=\"prob\")\n",
    "        if isSave:\n",
    "            with open(te_files, 'r') as f1, open('sample.unitest', \"w\") as f2:\n",
    "                for i in range(numToSave):\n",
    "                    sample = f1.readline()\n",
    "                    result = next(P_G)\n",
    "                    pred = str(result['prob'])\n",
    "                    f2.write('\\t'.join([pred, sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing model cleaned at ./tmp/checkpoint/EMNN\n",
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/checkpoint/EMNN', '_tf_random_seed': 2020, '_save_summary_steps': 10000, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "  visible_device_list: \"0\"\n",
      "}\n",
      "allow_soft_placement: true\n",
      "log_device_placement: true\n",
      ", '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09d44959b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Parsing ./test_nn_seq/tfdata.train\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "Parsing ./test_nn_seq/tfdata.val\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/checkpoint/EMNN/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./tmp/checkpoint/EMNN/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2020-05-15T15:42:11Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-05-15-15:42:33\n",
      "INFO:tensorflow:Saving dict for global step 0: acc_age = 0.06531915, auc_gender = 0.49917835, global_step = 0, loss = 1.6592884\n",
      "INFO:tensorflow:loss = 1.6578788, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.289909\n",
      "INFO:tensorflow:loss = 1.4858401, step = 101 (344.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.297528\n",
      "INFO:tensorflow:loss = 1.1948217, step = 201 (336.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.298799\n",
      "INFO:tensorflow:loss = 1.4449173, step = 301 (334.673 sec)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"creative_size\":3412772,\n",
    "    \"ad_size\":2264191,\n",
    "    \"product_size\":33273,\n",
    "    \"product_category_size\":19,\n",
    "    \"advertiser_size\":52091,\n",
    "    \"industry_size\":326,\n",
    "    \"seq_len\":60,\n",
    "\n",
    "    \"creative_emb_size\":128,      # 1G左右\n",
    "    \"ad_emb_size\":128,            # 1G左右\n",
    "    \"product_emb_size\":32,\n",
    "    \"product_category_emb_size\":4,\n",
    "    \"advertiser_emb_size\":32,\n",
    "    \"industry_emb_size\":4,\n",
    "    \n",
    "    \"a_size\":4,\n",
    "    \"num_class\":10,\n",
    "    \"b_layers\":[32,16],\n",
    "    \"t_layers\":8,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"val_itrs\": 1000\n",
    "}\n",
    "\n",
    "run_config = setConfig(dynamic_alloc=True, gpu_no='0', save_checkpoints_steps=1000, print_steps=100, seed=2020)\n",
    "model = EMNN(run_config,params)\n",
    "model.compile(clear_existing_model=False)\n",
    "model.train_and_evaluate(\"./test_nn_seq/tfdata.train\",\"./test_nn_seq/tfdata.val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36_Tensorflow",
   "language": "python",
   "name": "python36_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
